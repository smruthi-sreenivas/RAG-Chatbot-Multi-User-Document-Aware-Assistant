

# ğŸ¤– RAG Chatbot â€“ Multi-User Document-Aware Assistant

A powerful **Retrieval-Augmented Generation (RAG)** chatbot that enables multiple users to upload documents (PDFs) or crawl websites, index the content in **Elasticsearch**, and chat with the data using **Ollama LLMs** â€” all with session persistence via **Redis**.

---

## ğŸš€ Features

- ğŸ” **Multi-User Authentication** (username-based)
- ğŸ’¬ **Session Management** (create, switch, delete conversations)
- ğŸ§  **RAG Pipeline**
  - PDF ingestion & chunking
  - Webpage crawling & indexing
  - Embeddings with Ollama (`nomic-embed-text`)
  - Vector search powered by Elasticsearch
- ğŸ—£ï¸ **Conversational Memory** via Redis
- ğŸ§© **Customizable Parameters**
  - Chunk size, overlap, top_k, and temperature
- ğŸ§¹ **Document Management**
  - Upload, list, delete, or filter indexed documents
- ğŸŒ **Web Crawling** up to depth 2
- âš™ï¸ **Streamlit Interface** with clean UI

---

## ğŸ§° Tech Stack

| Component | Technology |
|------------|-------------|
| **Frontend / UI** | Streamlit |
| **LLM** | Ollama (`phi3:mini`) |
| **Embeddings** | `nomic-embed-text` |
| **Vector Store** | Elasticsearch |
| **Memory Store** | Redis |
| **Document Loader** | LangChain PyPDFLoader |
| **Text Splitter** | RecursiveCharacterTextSplitter |

---

## ğŸ› ï¸ Setup Instructions

### 1ï¸âƒ£ Clone the Repository

git clone https://github.com/<your-username>/rag-chatbot.git

### Install Dependencies

pip install -r requirements.txt

### Start Required Services with Docker Compose

docker compose up -d

This will:
-ğŸ§  Start Redis â€“ used for storing chat memory and session data
-ğŸ“¦ Start Elasticsearch â€“ used as the vector store for document embeddings
-ğŸŒ Start Elasticvue â€“ a web-based GUI for managing and exploring your Elasticsearch indices
-ğŸ§© Connect to Ollama â€“ runs locally to provide the LLM (phi3:mini) and embeddings (nomic-embed-text) for your RAG pipeline

To stop everything:

docker compose down

ğŸ§© Ollama Setup

Make sure Ollama is installed and running locally, then pull your required models:
ollama pull phi3:mini
ollama pull nomic-embed-text

